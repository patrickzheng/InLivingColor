{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.9 (default, Mar  9 2015 16:20:48)\n",
      "SparkContext available as sc, HiveContext available as sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\"--num-executors 10 --master yarn --deploy-mode client --executor-memory 1500m --driver-memory 1500m\"\n",
    "\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flickr API_KEY:  ea199 ...\n",
      "Using AWS_ACCESS_KEY_ID:  AKIAJ ...\n"
     ]
    },
    {
     "ename": "S3ResponseError",
     "evalue": "S3ResponseError: 403 Forbidden\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mS3ResponseError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92c3e1410207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0ms3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbucket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS3_BUCKET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/boto/s3/connection.pyc\u001b[0m in \u001b[0;36mget_bucket\u001b[1;34m(self, bucket_name, validate, headers)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \"\"\"\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_bucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbucket_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/boto/s3/connection.pyc\u001b[0m in \u001b[0;36mhead_bucket\u001b[1;34m(self, bucket_name, headers)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'AccessDenied'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Access Denied'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[1;31m# For backward-compatibility, we'll populate part of the exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mS3ResponseError\u001b[0m: S3ResponseError: 403 Forbidden\n"
     ]
    }
   ],
   "source": [
    "from _configuration import *\n",
    "\n",
    "import time\n",
    "import boto\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "s3 = boto.connect_s3()\n",
    "bucket = s3.get_bucket(S3_BUCKET)\n",
    "\n",
    "\n",
    "def batch_aggregate_small_s3files_bydatetimebin_onto_s3(collection=None,firstdatetimebin=FIRSTBINEVER, dry_run=True):\n",
    "\n",
    "    print '------------------------------------------------------------------------'\n",
    "    print 'Beginning Batch Job of Aggregating Small files on S3 and storing on S3'\n",
    "    print ''\n",
    "    print 'Initial Scan:'\n",
    "\n",
    "    datetimebin = firstdatetimebin + 3600*80\n",
    "    \n",
    "\n",
    "    while datetimebin < time.time() - 3600:\n",
    "\n",
    "        datetimebinstr = time.strftime('%Y-%m-%d_%H', time.gmtime(datetimebin))\n",
    "        datetimebin += 3600\n",
    "        \n",
    "#         print time.time(), datetimebin\n",
    "        \n",
    "        key = bucket.get_key('%s/metaplus_%s.json/_SUCCESS'%(collection,datetimebinstr))\n",
    "\n",
    "        # If YYYY-MM-DD_HH.p has already been generated, no need to do it again.\n",
    "        if key is not None:\n",
    "            print '--- File %s already exists. Skipping...' % (datetimebinstr+'.json')\n",
    "            continue\n",
    "\n",
    "        print datetimebinstr, ': Might need to aggregate'\n",
    "        \n",
    "\n",
    "\n",
    "        # Delete files that might prevent us from writing\n",
    "        filestodelete = list(bucket.list(os.path.join(collection,'metaplus_%s.json'%(datetimebinstr))))\n",
    "        if len(filestodelete) > 0:\n",
    "            print '--- Deleting:', filestodelete\n",
    "            bucket.delete_keys(filestodelete)\n",
    "        \n",
    "        if dry_run is True:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Open files like s3n://...@bucket/collection/2015-06-21_17/*metaplus.json    \n",
    "        a = sc.textFile(os.path.join(S3_PREFIX,datetimebinstr,'*metaplus.json'))\n",
    "        # This will throw an exception if there is no file\n",
    "#         a.first()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Coalesce into a small number of partitions so that each file piece is ~100MB\n",
    "        a = a.coalesce(30, shuffle=True)\n",
    "#         a.persist()\n",
    "#         numberofpartitions = int(ceil(a.map(len).reduce(lambda x,y: x+y)/(100.0*1024*1024)))\n",
    "#         a = a.coalesce(int(numberofpartitions), shuffle=True)\n",
    "\n",
    "        %time a.saveAsTextFile(os.path.join(S3_PREFIX,'metaplus_%s.json'%(datetimebinstr)))    \n",
    "        print \"Saved to S3\"\n",
    "\n",
    "\n",
    "\n",
    "batch_aggregate_small_s3files_bydatetimebin_onto_s3(collection=collection)\n",
    "\n",
    "print 'Beginning batch job in 5 seconds...'\n",
    "\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take individual metaplus.json files, aggregate, and copy to S3 (for convenience) and HDFS (for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### print \"---------------------------------------------\"\n",
    "print \"STAGE 1: Aggregating small S3 files and copying to S3\"\n",
    "\n",
    "batch_aggregate_small_s3files_bydatetimebin_onto_s3(collection=collection, dry_run=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
