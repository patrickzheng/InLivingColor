{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "spark_home = os.environ.get('SPARK_HOME', None)\n",
      "if not spark_home:\n",
      "    raise ValueError('SPARK_HOME environment variable is not set')\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
      "# Make sure this file exists\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"\\\n",
      "--jars /home/ubuntu/InLivingColor/server/pyspark-cassandra.jar \\\n",
      "--driver-class-path  /home/ubuntu/InLivingColor/server/pyspark-cassandra.jar \\\n",
      "--executor-memory 1500m \\\n",
      "--master yarn --deploy-mode client \\\n",
      "--py-files /home/ubuntu/InLivingColor/server/_configuration.py \\\n",
      "    \"\"\"\n",
      "# --py-files /home/ubuntu/InLivingColor/server/pyspark_cassandra-0.1.5-py2.7.egg \\\n",
      "# --conf spark.cassandra.connection.host='172.31.6.181,172.31.6.1\n",
      "# 82,172.31.6.183,172.31.6.184,172.31.6.185,172.31.6.186,172.31.6.187,172.31.6.188' \\\n",
      "#     yourscript.py\n",
      "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n",
      "# \n",
      "# from pyspark import SparkContext\n",
      "# # sc = SparkContext(\"local\", \"App Name\", pyFiles=['MyFile.py', 'lib.zip', 'app.egg'])\n",
      "# sc = SparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
        "      /_/\n",
        "\n",
        "Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)\n",
        "SparkContext available as sc, HiveContext available as sqlCtx.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MY_BUCKET = 'inlivingcolor'\n",
      "\n",
      "# AWS1 = 'AKIAJQNKVDFSWHKQGRAQ'\n",
      "# AWS2 = 'Z4rjaxP6uk3DaZGnDKRbRWDZjIwYsK+5y3zWZ3gI'\n",
      "\n",
      "from _configuration import *\n",
      "\n",
      "def GetS3Keys(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, query):\n",
      "    from boto.s3.connection import S3Connection\n",
      "#     from boto.s3.key import Key\n",
      "    import sys\n",
      "    \n",
      "    conn = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
      "    bucket = conn.get_bucket('inlivingcolor')\n",
      "    keys = bucket.list(query)\n",
      "    return [key.name for key in keys]\n",
      "\n",
      "def WriteEmptyS3File(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, key):\n",
      "    from boto.s3.connection import S3Connection\n",
      "#     from boto.s3.key import Key\n",
      "    import sys\n",
      "    \n",
      "    conn = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
      "    bucket = conn.get_bucket('inlivingcolor')\n",
      "    k = bucket.new_key(key)\n",
      "    k.set_contents_from_string('')\n",
      "    return key\n",
      "\n",
      "collection = 'allrecent'\n",
      "\n",
      "import os\n",
      "prefixes = [os.path.join(collection,'%02d'%i) for i in range(100)]\n",
      "print \"Using prefixes: %s ...\" % str(prefixes[:5])\n",
      "rdd = sc.parallelize(prefixes)\\\n",
      "    .flatMap(lambda i: GetS3Keys(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,i))\\\n",
      "    .filter(lambda s: s.endswith('exif.json'))\\\n",
      "    .map(lambda s: s[:-9]+'DOWNLOAD_AND_WRITE_SUCCEEDED')\\\n",
      "    .map(lambda key: WriteEmptyS3File(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,key))\n",
      "print rdd.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'hi'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "INPUT_KEY_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLKeyConverter\"\n",
      "INPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLValueConverter\"\n",
      "OUTPUT_KEY_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLKeyConverter\"\n",
      "OUTPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLValueConverter\"\n",
      "\n",
      "# Reading from Cassandra\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"52.8.159.159\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"flickrsot\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"50\"\n",
      "}\n",
      "cass_rdd = sc.newAPIHadoopRDD(\n",
      "    # inputFormatClass\n",
      "    \"org.apache.cassandra.hadoop.cql3.CqlInputFormat\",\n",
      "    # keyClass\n",
      "    \"java.util.Map\",\n",
      "    # valueClass\n",
      "    \"java.util.Map\",\n",
      "    keyConverter=INPUT_KEY_CONVERTER,\n",
      "    valueConverter=INPUT_VALUE_CONVERTER,\n",
      "    conf=conf)\n",
      "# cass_rdd.coun\n",
      "print cass_rdd.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 304 in stage 9.0 failed 4 times, most recent failure: Lost task 304.3 in stage 9.0 (TID 795, ip-172-31-6-188.us-west-1.compute.internal): java.lang.IllegalArgumentException: user_id is not a column defined in this metadata\n\tat com.datastax.driver.core.ColumnDefinitions.getAllIdx(ColumnDefinitions.java:273)\n\tat com.datastax.driver.core.ColumnDefinitions.getFirstIdx(ColumnDefinitions.java:279)\n\tat com.datastax.driver.core.ArrayBackedRow.getIndexOf(ArrayBackedRow.java:69)\n\tat com.datastax.driver.core.AbstractGettableData.getString(AbstractGettableData.java:127)\n\tat com.parsely.spark.converters.FromUsersCQLValueConverter.convert(SparkConverters.scala:108)\n\tat com.parsely.spark.converters.FromUsersCQLValueConverter.convert(SparkConverters.scala:98)\n\tat org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:188)\n\tat org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:188)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:312)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$33.apply(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD$$anonfun$33.apply(RDD.scala:1177)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1503)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1503)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-1c4a04e9e333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_KEY_CONVERTER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_VALUE_CONVERTER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     conf=conf)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# cass_rdd.coun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcass_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mnewAPIHadoopRDD\u001b[0;34m(self, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[1;32m    547\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopRDD(self._jsc, inputFormatClass, keyClass,\n\u001b[1;32m    548\u001b[0m                                                    \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                                                    jconf, batchSize)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 304 in stage 9.0 failed 4 times, most recent failure: Lost task 304.3 in stage 9.0 (TID 795, ip-172-31-6-188.us-west-1.compute.internal): java.lang.IllegalArgumentException: user_id is not a column defined in this metadata\n\tat com.datastax.driver.core.ColumnDefinitions.getAllIdx(ColumnDefinitions.java:273)\n\tat com.datastax.driver.core.ColumnDefinitions.getFirstIdx(ColumnDefinitions.java:279)\n\tat com.datastax.driver.core.ArrayBackedRow.getIndexOf(ArrayBackedRow.java:69)\n\tat com.datastax.driver.core.AbstractGettableData.getString(AbstractGettableData.java:127)\n\tat com.parsely.spark.converters.FromUsersCQLValueConverter.convert(SparkConverters.scala:108)\n\tat com.parsely.spark.converters.FromUsersCQLValueConverter.convert(SparkConverters.scala:98)\n\tat org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:188)\n\tat org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:188)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:312)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$33.apply(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD$$anonfun$33.apply(RDD.scala:1177)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1503)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1503)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize(range(50))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime as dt\n",
      "import sys\n",
      "from uuid import uuid4\n",
      "\n",
      "from pyspark.context import SparkConf\n",
      "from pyspark_cassandra import CassandraSparkContext, saveToCassandra\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cassandra.cluster import Cluster\n",
      "\n",
      "cluster = Cluster()\n",
      "session = cluster.connect('inlivingcolor')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reading from Cassandra\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"localhost\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"users\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"5000\"\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<cassandra.cluster.Session at 0x7f2c5c135a90>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "INPUT_KEY_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLKeyConverter\"\n",
      "INPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLValueConverter\"\n",
      "OUTPUT_KEY_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLKeyConverter\"\n",
      "OUTPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLValueConverter\"\n",
      "\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"localhost\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"flickrsot\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"5000\"\n",
      "}\n",
      "cass_rdd = sc.newAPIHadoopRDD(\n",
      "    # inputFormatClass\n",
      "    \"org.apache.cassandra.hadoop.cql3.CqlInputFormat\",\n",
      "    # keyClass\n",
      "    \"java.util.Map\",\n",
      "    # valueClass\n",
      "    \"java.util.Map\",\n",
      "    keyConverter=INPUT_KEY_CONVERTER,\n",
      "    valueConverter=INPUT_VALUE_CONVERTER,\n",
      "    conf=conf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: java.lang.ClassNotFoundException: org.apache.cassandra.hadoop.cql3.CqlInputFormat\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:274)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:157)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:495)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:478)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-55732a18db71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_KEY_CONVERTER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_VALUE_CONVERTER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     conf=conf)\n\u001b[0m",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mnewAPIHadoopRDD\u001b[0;34m(self, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[1;32m    547\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopRDD(self._jsc, inputFormatClass, keyClass,\n\u001b[1;32m    548\u001b[0m                                                    \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                                                    jconf, batchSize)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: java.lang.ClassNotFoundException: org.apache.cassandra.hadoop.cql3.CqlInputFormat\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:274)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:157)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:495)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:478)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}