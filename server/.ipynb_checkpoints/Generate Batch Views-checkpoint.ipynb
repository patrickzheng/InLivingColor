{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Front End Matter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "spark_home = os.environ.get('SPARK_HOME', None)\n",
      "if not spark_home:\n",
      "    raise ValueError('SPARK_HOME environment variable is not set')\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
      "# Make sure this file exists\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"\\\n",
      "--jars /home/ubuntu/InLivingColor/server/pyspark-cassandra.jar \\\n",
      "--driver-class-path  /home/ubuntu/InLivingColor/server/pyspark-cassandra.jar \\\n",
      "--num-executors 8 \\\n",
      "--executor-cores 2 \\\n",
      "--executor-memory 1500m \\\n",
      "--driver-memory 1500m \\\n",
      "--master yarn --deploy-mode client \\\n",
      "--py-files /home/ubuntu/InLivingColor/server/_configuration.py \\\n",
      "--py-files /home/ubuntu/InLivingColor/server/flickr_helper.py \\\n",
      "    \"\"\"\n",
      "# --py-files /home/ubuntu/InLivingColor/server/pyspark_cassandra-0.1.5-py2.7.egg \\\n",
      "# --conf spark.cassandra.connection.host='172.31.6.181,172.31.6.1\n",
      "# 82,172.31.6.183,172.31.6.184,172.31.6.185,172.31.6.186,172.31.6.187,172.31.6.188' \\\n",
      "#     yourscript.py\n",
      "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n",
      "# \n",
      "# from pyspark import SparkContext\n",
      "# # sc = SparkContext(\"local\", \"App Name\", pyFiles=['MyFile.py', 'lib.zip', 'app.egg'])\n",
      "# sc = SparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
        "      /_/\n",
        "\n",
        "Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)\n",
        "SparkContext available as sc, HiveContext available as sqlCtx.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MY_BUCKET = 'inlivingcolor'\n",
      "    \n",
      "from _configuration import *\n",
      "\n",
      "\n",
      "def json2dict(string):\n",
      "    import json\n",
      "    try:\n",
      "        return json.loads(string)\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "def dict2json(string):\n",
      "    import json\n",
      "    try:\n",
      "        return json.dumps(string)\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "def concatdicts(d1,d2):\n",
      "    d1.update(d2)\n",
      "    try:\n",
      "        return d1\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "def GetS3Keys(query):\n",
      "    from boto.s3.connection import S3Connection\n",
      "    \n",
      "    conn = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
      "    keys = conn.get_bucket('inlivingcolor').list(query)\n",
      "    return [key.name for key in keys]\n",
      "\n",
      "def WriteEmptyS3File(key):\n",
      "    from boto.s3.connection import S3Connection\n",
      "    \n",
      "    try:\n",
      "        conn = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
      "        k = conn.get_bucket('inlivingcolor').new_key(key)\n",
      "        k.set_contents_from_string('')\n",
      "        return key\n",
      "    except:\n",
      "        return False    \n",
      "def GetS3Value(keyname):\n",
      "    from boto.s3.connection import S3Connection\n",
      "    \n",
      "    conn = S3Connection(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
      "    contents = conn.get_bucket('inlivingcolor').get_key(keyname).get_contents_as_string()\n",
      "    return contents\n",
      "\n",
      "# from http://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys\n",
      "def flatten(d, parent_key='', sep='_'):\n",
      "    import collections\n",
      "    items = []\n",
      "    for k, v in d.items():\n",
      "        new_key = parent_key + sep + k if parent_key else k\n",
      "        if isinstance(v, collections.MutableMapping):\n",
      "            items.extend(flatten(v, new_key, sep=sep).items())\n",
      "        else:\n",
      "            items.append((new_key, v))\n",
      "    return dict(items)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using flickr API_KEY:  ea199ca8826cf44856369f24ae18feb9\n",
        "Using AWS_ACCESS_KEY_ID:  AKIAJQNKVDFSWHKQGRAQ\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "%time GetS3Value('allrecent/00/17952410543/image.jpg')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 20.3 ms, sys: 13.3 ms, total: 33.6 ms\n",
        "Wall time: 122 ms\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Create and persist RDD of S3 keys that end in 'metaplus.json'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "collection = 'has_geo'\n",
      "prefixes = [os.path.join(collection,'%02d'%i) for i in range(0,100)]\n",
      "print \"Using prefixes: %s ...\" % str(prefixes[:5])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Using prefixes: ['has_geo/00', 'has_geo/01', 'has_geo/02', 'has_geo/03', 'has_geo/04'] ...\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rddmetapluskeys = sc.parallelize(prefixes[:],30)\\\n",
      "                .flatMap(lambda i: GetS3Keys(i))\\\n",
      "                .filter(lambda s: s.endswith('metaplus.json'))\\\n",
      "                .persist()\n",
      "\n",
      "print 'Number of partitions:', rddmetapluskeys.getNumPartitions()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of partitions: 30\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        \n",
      "%time rddmetapluskeys.count()\n",
      "# GetS3Value('recent/00/18289665554/metaplus.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 7.64 ms, sys: 3.1 ms, total: 10.7 ms\n",
        "Wall time: 20.3 s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "195888"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Copy and aggregate 'metaplus.json' data to S3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hdfs dfs -rm -f -R /inlivingcolor/has_geo*\n",
      "!hdfs dfs -ls /inlivingcolor/\n",
      "\n",
      "print \"Using collection:\", collection\n",
      "\n",
      "rdd = sc.textFile(\"s3n://inlivingcolor/%s/00/*/metaplus.json\",minPartitions=28).coalesce(100)\n",
      "# %time print rdd.count()\n",
      "%time rdd.saveAsTextFile('hdfs://52.8.159.159/inlivingcolor/%s-metaplus.json'%collection)\n",
      "\n",
      "!hdfs dfs -ls /inlivingcolor/has_geo-metaplus.json\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 items\r\n",
        "drwxr-xr-x   - ubuntu supergroup          0 2015-06-19 06:43 /inlivingcolor/omega-metaplus.json\r\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute '_get_object_id'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-2fb809f019a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Using collection:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s3n://inlivingcolor/%s/%02d/*/metaplus.json\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# %time print rdd.count()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"time rdd.saveAsTextFile('hdfs://52.8.159.159/inlivingcolor/%s-metaplus.json'%collection)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[0m\u001b[1;32m    381\u001b[0m                    UTF8Deserializer(use_unicode))\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         args_command = ''.join(\n\u001b[0;32m--> 529\u001b[0;31m                 [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mcommand_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_get_object_id'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using collection: has_geo\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure the file is working\n",
      "rddmetadict = sc.textFile('hdfs://52.8.159.159/inlivingcolor/omega-metaplus.json')\\\n",
      "                .map(json2dict)\n",
      "rddmetadict.persist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "PythonRDD[60] at RDD at PythonRDD.scala:42"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time print rddmetadict.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "71230\n",
        "CPU times: user 1min 10s, sys: 15 s, total: 1min 25s\n",
        "Wall time: 11min 42s\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writes empty COPIED_TO_HDFS file on S3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Writes empty COPIED_TO_HDFS file on S3\n",
      "rdd = rddmetadict\n",
      "# rdd = rdd.map(lambda x: x['info']['id'])\n",
      "# rdd = rdd.map(lambda x: x['collection'])\n",
      "rdd = rdd.map(lambda d: '{collection}/{secret:02d}/{photoid}/COPIED_TO_HDFS'.format(**d))\n",
      "rdd.foreach(WriteEmptyS3File)\n",
      "# print(\"Randy's phone is {Randy[phone]}\".format(**people))\n",
      "rdd.first()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maps = {}\n",
      "maps['collection'] = lambda d: d['collection']\n",
      "maps['photoid'] = lambda d: d['photoid']\n",
      "maps['datetaken'] = lambda d: d['info_dates_taken']\n",
      "\n",
      "maps['C1C1R'] = lambda d: int(d['clusters_1_centroids_1'][0]*256)\n",
      "maps['C1C1G'] = lambda d: int(d['clusters_1_centroids_1'][1]*256)\n",
      "maps['C1C1B'] = lambda d: int(d['clusters_1_centroids_1'][2]*256)\n",
      "maps['C1P1'] = lambda d: int(d['clusters_1_probs_1']*100)\n",
      "\n",
      "\n",
      "# Use the following to generate the above\n",
      "# for i in range(1,2):\n",
      "#     for j in range(1,i+1):\n",
      "#         print \"maps['C%dC%dR'] = lambda d: d['clusters_%d_centroids_%d'][0]\"%(i,j,i,j)\n",
      "#         print \"maps['C%dC%dG'] = lambda d: d['clusters_%d_centroids_%d'][1]\"%(i,j,i,j)\n",
      "#         print \"maps['C%dC%dB'] = lambda d: d['clusters_%d_centroids_%d'][2]\"%(i,j,i,j)\n",
      "#         print \"maps['C%dP%d'] = lambda d: d['clusters_%d_probs_%d']\"%(i,j,i,j)\n",
      "\n",
      "\n",
      "def CreateCassandraDict(d):\n",
      "#     return {k:}\n",
      "    output = {}\n",
      "    \n",
      "    for k,v in maps.iteritems():\n",
      "        try:\n",
      "            output[k] = v(d)\n",
      "        except:\n",
      "            output[k] = \"\"\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "rdd = rddmetadict\n",
      "rdd = rdd.map(flatten)\n",
      "rdd = rdd.map(CreateCassandraDict)\n",
      "rdd.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-44-f5fa51685278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCreateCassandraDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \"\"\"\n\u001b[0;32m-> 1243\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m    538\u001b[0m                 self.target_id, self.name)\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    428\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AddToCassandra(d):\n",
      "    from cqlengine import columns\n",
      "    from cqlengine.models import Model\n",
      "    from cqlengine import connection\n",
      "    \n",
      "    # Define a model\n",
      "    class demo3test(Model):\n",
      "        collection = columns.Text(primary_key=True)\n",
      "        datetaken = columns.Text(primary_key=True)\n",
      "        photoid = columns.Text(primary_key=True)\n",
      "        C1C1R = columns.Integer()\n",
      "        C1C1G = columns.Integer()\n",
      "        C1C1B = columns.Integer()\n",
      "        C1P1 = columns.Integer()\n",
      "        lucene = columns.Text()\n",
      "    \n",
      "        def __repr__(self):\n",
      "            return '<collection=%s photoid=%s>' % (self.collection, self.photoid)\n",
      "        \n",
      "    connection.setup(['127.0.0.1'], \"inlivingcolor\")\n",
      "    \n",
      "    from cqlengine.management import sync_table\n",
      "    sync_table(demo3test)\n",
      "    \n",
      "    demo3test.create(**d)\n",
      "    \n",
      "AddToCassandra(rdd.first())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "CREATE CUSTOM INDEX demo3test_index ON demo3test (lucene) \n",
      "USING 'com.stratio.cassandra.lucene.Index'\n",
      "WITH OPTIONS = {\n",
      "    'refresh_seconds' : '1',\n",
      "    'schema' : '{\n",
      "        fields : {\n",
      "              datetaken : {type : \"date\", pattern : \"yyyy/MM/dd\"},\n",
      "              photoid : {type : \"integer\"},\n",
      "              C1C1R : {type : \"integer\"},\n",
      "              C1C1G : {type : \"integer\"},\n",
      "              C1C1B : {type : \"integer\"},\n",
      "              C1P1  : {type : \"integer\"},\n",
      "            id    : {type : \"integer\"},\n",
      "            user  : {type : \"string\"},\n",
      "            body  : {type : \"text\", analyzer : \"english\"},\n",
      "            time  : {type : \"date\", pattern : \"yyyy/MM/dd\"},\n",
      "            place : {type : \"geo_point\", latitude:\"latitude\", longitude:\"longitude\"}\n",
      "        }\n",
      "    }'\n",
      "};\n",
      "\n",
      "CREATE CUSTOM INDEX demo3test_index ON demo3test (lucene) \n",
      "USING 'com.stratio.cassandra.lucene.Index'\n",
      "WITH OPTIONS = {\n",
      "    'refresh_seconds' : '1',\n",
      "    'schema' : '{\n",
      "        fields : {\n",
      "              C1C1R : {type : \"integer\"},\n",
      "              C1C1G : {type : \"integer\"},\n",
      "              C1C1B : {type : \"integer\"},\n",
      "              C1P1  : {type : \"integer\"}\n",
      "              datetaken  : {type : \"date\", pattern : \"yyyy/MM/dd\"},\n",
      "        }\n",
      "    }'\n",
      "};\n",
      "\n",
      "SELECT * FROM demo3test WHERE lucene='{\n",
      "    filter : {type:\"range\", field:\"C1C1B\", lower:\"4\", upper:\"90\"},\n",
      "    filter : {type:\"range\", field:\"C1C1G\", lower:\"10\", upper:\"12\"},\n",
      "    filter : {type:\"range\", field:\"C1C1R\", lower:\"4\", upper:\"12\"}\n",
      "}' limit 100;\n",
      "\n",
      "SELECT * FROM demo3test WHERE lucene='{\n",
      "    filter : {type:\"range\", field:\"C1C1\", lower:\"4\", upper:\"67\"},\n",
      "    query  : {type:\"phrase\", field:\"body\", value:\"big data gives organizations\", slop:1}\n",
      "}' limit 100;"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.foreach(AddToCassandra)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o286.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 19.0 failed 4 times, most recent failure: Lost task 6.3 in stage 19.0 (TID 238, ip-172-31-6-184.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-5.4.2-1.cdh5.4.2.p0.2/jars/spark-assembly-1.3.0-cdh5.4.2-hadoop2.6.0-cdh5.4.2.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-5.4.2-1.cdh5.4.2.p0.2/jars/spark-assembly-1.3.0-cdh5.4.2-hadoop2.6.0-cdh5.4.2.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 270, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 675, in processPartition\n    f(x)\n  File \"<ipython-input-35-d1be5d42e443>\", line 25, in AddToCassandra\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/models.py\", line 551, in create\n    return cls.objects.create(**kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/query.py\", line 637, in create\n    return self.model(**kwargs).batch(self._batch).ttl(self._ttl).\\\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/models.py\", line 337, in __init__\n    value = column.to_python(value)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/columns.py\", line 277, in to_python\n    return self.validate(value)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/columns.py\", line 274, in validate\n    raise ValidationError(\"{} {} can't be converted to integral value\".format(self.column_name, value))\nValidationError: C1C1R  can't be converted to integral value\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-37-b193c5c7027d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddToCassandra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mforeach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \"\"\"\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \"\"\"\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mbytesInJava\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o286.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 19.0 failed 4 times, most recent failure: Lost task 6.3 in stage 19.0 (TID 238, ip-172-31-6-184.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-5.4.2-1.cdh5.4.2.p0.2/jars/spark-assembly-1.3.0-cdh5.4.2-hadoop2.6.0-cdh5.4.2.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-5.4.2-1.cdh5.4.2.p0.2/jars/spark-assembly-1.3.0-cdh5.4.2-hadoop2.6.0-cdh5.4.2.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 2253, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 270, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/rdd.py\", line 675, in processPartition\n    f(x)\n  File \"<ipython-input-35-d1be5d42e443>\", line 25, in AddToCassandra\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/models.py\", line 551, in create\n    return cls.objects.create(**kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/query.py\", line 637, in create\n    return self.model(**kwargs).batch(self._batch).ttl(self._ttl).\\\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/models.py\", line 337, in __init__\n    value = column.to_python(value)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/columns.py\", line 277, in to_python\n    return self.validate(value)\n  File \"/usr/local/lib/python2.7/dist-packages/cqlengine/columns.py\", line 274, in validate\n    raise ValidationError(\"{} {} can't be converted to integral value\".format(self.column_name, value))\nValidationError: C1C1R  can't be converted to integral value\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "INPUT_KEY_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLKeyConverter\"\n",
      "INPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLValueConverter\"\n",
      "OUTPUT_KEY_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLKeyConverter\"\n",
      "OUTPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLValueConverter\"\n",
      "\n",
      "# Reading from Cassandra\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"52.8.159.159\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"flickrsot\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"50\"\n",
      "}\n",
      "cass_rdd = sc.newAPIHadoopRDD(\n",
      "    # inputFormatClass\n",
      "    \"org.apache.cassandra.hadoop.cql3.CqlInputFormat\",\n",
      "    # keyClass\n",
      "    \"java.util.Map\",\n",
      "    # valueClass\n",
      "    \"java.util.Map\",\n",
      "    keyConverter=INPUT_KEY_CONVERTER,\n",
      "    valueConverter=INPUT_VALUE_CONVERTER,\n",
      "    conf=conf)\n",
      "# cass_rdd.coun\n",
      "print cass_rdd.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cassandra.cluster import Cluster\n",
      "\n",
      "cluster = Cluster()\n",
      "session = cluster.connect('inlivingcolor')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reading from Cassandra\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"localhost\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"users\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"5000\"\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "INPUT_KEY_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLKeyConverter\"\n",
      "INPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.FromUsersCQLValueConverter\"\n",
      "OUTPUT_KEY_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLKeyConverter\"\n",
      "OUTPUT_VALUE_CONVERTER = \"com.parsely.spark.converters.ToCassandraCQLValueConverter\"\n",
      "\n",
      "conf = {\n",
      "    \"cassandra.input.thrift.address\": \"localhost\",\n",
      "    \"cassandra.input.thrift.port\": \"9160\",\n",
      "    \"cassandra.input.keyspace\": 'inlivingcolor',\n",
      "    \"cassandra.input.columnfamily\": \"flickrsot\",\n",
      "    \"cassandra.input.partitioner.class\":\"Murmur3Partitioner\",\n",
      "    \"cassandra.input.page.row.size\": \"5000\"\n",
      "}\n",
      "cass_rdd = sc.newAPIHadoopRDD(\n",
      "    # inputFormatClass\n",
      "    \"org.apache.cassandra.hadoop.cql3.CqlInputFormat\",\n",
      "    # keyClass\n",
      "    \"java.util.Map\",\n",
      "    # valueClass\n",
      "    \"java.util.Map\",\n",
      "    keyConverter=INPUT_KEY_CONVERTER,\n",
      "    valueConverter=INPUT_VALUE_CONVERTER,\n",
      "    conf=conf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}